# Deep Dive into the Mathematics of Transformer
The paper on transformer [Attention is All You Need](#attention-is-all-you-need) by Vaswani, et al. (2017) detailed the explanation of how the transformer architecture works. 
All modern large language models (LLMs) in the field of language processing primarily utilize the transformer architecture. However, the fundamental working of the transformer 
architecture is dependent on the concept of the attention. We will deep dive into this concept and try and understand what is attention and why is it the engine of this architecture.

# Attention in Transformer


# References
* <a name="attention-is-all-you-need"></a> [Attention is all you need] (https://arxiv.org/pdf/1706.03762)
* [A Mathematical Perspective on Transformers] (https://arxiv.org/pdf/2312.10794)
* [Transformers: a Primer] (https://www.columbia.edu/~jsl2239/transformers.html#dot_attention)
* Building Large Language Models from Scratch, Sebastian Raschka, Chapter 3
